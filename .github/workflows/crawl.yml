name: USCIS Changes

on:
  # run every hour
  schedule:
    - cron: '0 * * * *'
  # allow manual runs from the Actions tab
  workflow_dispatch:

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      # 1️⃣  Check out the repo
      - uses: actions/checkout@v4

      # 2️⃣  Set up Python 3.12
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      # 3️⃣  Install crawler dependencies
      - name: Install dependencies
        run: pip install --upgrade pip requests beautifulsoup4

      # 4️⃣  Run the crawler (writes to docs/, snapshots/, index.json)
      - name: Run crawler
        run: python crawler/crawler.py

      # 5️⃣  Commit any new crawl output back to main
      - name: Commit crawl results
        run: |
          git config user.name "uscis-bot"
          git config user.email "uscis-bot@example.com"
          git add docs snapshots index.json
          git commit -m "auto: crawl $(date -u)" || echo "Nothing to commit"
          git push

      # 6️⃣  Copy docs/ to the gh-pages branch for GitHub Pages
      - name: Publish docs folder to gh-pages
        if: github.ref == 'refs/heads/main'
        run: |
          # prepare or update the worktree
          git worktree add /tmp/gh-pages gh-pages || true
          rsync -a --delete docs/ /tmp/gh-pages/
          cd /tmp/gh-pages
          git add .
          git commit -m "deploy: $(date -u)" || echo "No changes in docs"
          git push origin gh-pages
